# Restate v1.6.0 Release Notes

## Highlights

- **Pause and Resume for Invocations**: Gain explicit control over invocation execution with new pause/resume capabilities. Invocations now pause by default when max retries are exhausted, allowing you to investigate issues before deciding to resume, restart, or kill.

- **DynamoDB Metadata Store**: Run Restate on AWS without the metadata-server role using Amazon DynamoDB as a metadata store provider, enabling simpler AWS deployments.

- **Restart Invocation from Journal Prefix**: Preserve partial progress when restarting invocations. Keep expensive work like completed calls, sleeps, and side effects instead of re-executing everything from the beginning.

- **Deployment Registration Improvements**: Safer defaults (`force=false`), idempotent registration, the new `breaking` flag, and an Update Deployment API make CI/CD pipelines more robust and production deployments safer.

- **Memory Management Improvements**: Automatic partition store memory balancing, optimized RocksDB defaults (2 GiB total, 85% memtables), and reduced query engine memory deliver better performance with a lower memory footprint out of the box.

---

## Table of Contents

- [Breaking Changes](#breaking-changes)
  - [Deprecated SDK Versions Now Rejected](#deprecated-sdk-versions-now-rejected)
  - [Retry Policy Configuration Migration](#retry-policy-configuration-migration)
  - [Helm Chart Default Resource Limits Changed](#helm-chart-default-resource-limits-changed)
  - [Automatic Partition Store Memory Management](#automatic-partition-store-memory-management)
  - [Journal Entry Size Limits Enforced](#journal-entry-size-limits-enforced)
  - [gRPC Message Size Configuration Renamed](#grpc-message-size-configuration-renamed)
  - [Prometheus Metrics Naming Convention Fixes](#prometheus-metrics-naming-convention-fixes)
- [Deprecations](#deprecations)
  - [/cluster-health Endpoint Deprecated](#cluster-health-endpoint-deprecated)
- [New Features](#new-features)
  - [Manual Pause and Resume for Invocations](#manual-pause-and-resume-for-invocations)
  - [DynamoDB Metadata Store Provider](#dynamodb-metadata-store-provider)
  - [Restart Invocation from Journal Prefix](#restart-invocation-from-journal-prefix)
  - [Deployment Registration and Management Improvements](#deployment-registration-and-management-improvements)
  - [Partition-Driven Log Trimming](#partition-driven-log-trimming)
  - [Batch Invocation Operations](#batch-invocation-operations)
  - [Network Configuration Overhaul](#network-configuration-overhaul)
  - [restate up Command](#restate-up-command)
  - [Kafka SASL OAUTHBEARER/OIDC Authentication](#kafka-sasl-oauthbeareroidc-authentication)
  - [Error Events Enabled by Default](#error-events-enabled-by-default)
  - [Snapshot Improvements](#snapshot-improvements)
  - [Snapshot Retention (Experimental)](#snapshot-retention-experimental)
  - [Kafka Batch Ingestion (Experimental)](#kafka-batch-ingestion-experimental)
- [Improvements](#improvements)
  - [Behavioral Changes](#behavioral-changes)
  - [Observability](#observability)
  - [Stability and Security](#stability-and-security)
  - [CLI Improvements](#cli-improvements)
  - [Helm Chart](#helm-chart)
- [Bug Fixes](#bug-fixes)

---

## Breaking Changes

### Deprecated SDK Versions Now Rejected

Starting with Restate 1.6, **new invocations will be rejected** for services deployed using the following deprecated SDK versions:

| SDK | Deprecated Versions |
|-----|---------------------|
| Java/Kotlin | < 2.0 |
| TypeScript | <= 1.4 |
| Go | < 0.16 |
| Python | < 0.6 |
| Rust | < 0.4 |

These SDK versions use Service Protocol V1-V4, which are no longer supported for new invocations. If you attempt to invoke a service using a deprecated SDK, you will receive error code **RT0020**.

**Note**: Existing in-flight invocations on deprecated deployments will continue to execute normally. Only new invocations are rejected.

#### Why This Matters

Older SDK versions are no longer recommended for production use. Upgrading ensures you benefit from the latest protocol improvements, bug fixes, and new features like:
- Deterministic random number generation via `ctx.rand()`
- Error metadata propagation
- Improved retry policies

#### Migration Guidance

If you encounter RT0020 errors:

1. **Identify affected deployments**: Use `restate deployments list` to see deployment protocol versions
2. **Upgrade your SDK** to a supported version:
   - [Java/Kotlin SDK](https://github.com/restatedev/sdk-java) >= 2.0
   - [TypeScript SDK](https://github.com/restatedev/sdk-typescript) >= 1.5
   - [Go SDK](https://github.com/restatedev/sdk-go) >= 0.16
   - [Python SDK](https://github.com/restatedev/sdk-python) >= 0.6
   - [Rust SDK](https://github.com/restatedev/sdk-rust) >= 0.4
3. **Re-register your deployment**: After upgrading, register the new deployment with Restate

**Related Issues:**
- [#3844](https://github.com/restatedev/restate/pull/3844): Enable Service Protocol v6
- [#3877](https://github.com/restatedev/restate/pull/3877): Don't allow requests to deployments using old protocol

---

### Retry Policy Configuration Migration

The deprecated `retry-policy` configuration option under `worker.invoker` has been removed. Users must migrate to the `default-retry-policy` option under the `invocation` section.

#### Why This Matters

The new retry policy configuration provides:
- More intuitive configuration location under `invocation`
- New `on-max-attempts` behavior option (pause or kill)
- Per-service and per-handler retry policy overrides via SDK configuration

#### Impact on Users

- **Existing configuration**: If you configured `worker.invoker.retry-policy`, Restate will ignore this option
- **Default behavior change**: Invocations now **pause** by default when max attempts are reached, instead of being killed as it happened with the old `retry-policy`

#### Migration Guidance

**Before (no longer supported):**
```toml
[worker.invoker]
retry-policy = { type = "exponential", initial-interval = "50ms", factor = 2.0, max-attempts = 200, max-interval = "20s" }
```

**After:**
```toml
[invocation.default-retry-policy]
initial-interval = "50ms"
exponentiation-factor = 2.0
max-attempts = 200
max-interval = "20s"
on-max-attempts = "kill"  # Use "kill" to match previous behavior, or "pause" (new default)
```

**New defaults:**

| Parameter        | Old Default     | New Default |
|------------------|-----------------|-------------|
| `max-attempts`   | 200             | 70          |
| `max-interval`   | 20s             | 60s         |
| `on-max-attempts` | kill (implicit) | pause       |

**Per-service overrides**: Retry policies can also be customized per service or handler using the respective SDK APIs. See the [Restate documentation on retries](https://docs.restate.dev/services/configuration#retries) for more details.

**Related Issues:**
- [#3911](https://github.com/restatedev/restate/pull/3911): Remove old invoker retry policy

---

### Helm Chart Default Resource Limits Changed

The Restate Helm chart default resource limits have been significantly increased to provide better out-of-the-box performance:

| Resource | Previous Default | New Default |
|----------|------------------|-------------|
| Memory limit | 3Gi | **8Gi** |
| Memory request | 1Gi | **8Gi** |
| CPU limit | 1 | **6** |
| CPU request | 500m | **4** |
| `RESTATE_ROCKSDB_TOTAL_MEMORY_SIZE` | (not set) | **3Gi** |

#### Why This Matters

**Scheduling impact**: The increased resource requests (8Gi memory, 4 CPU) mean pods may no longer be schedulable on nodes that previously had sufficient resources. This is especially relevant for:
- Development/test clusters with smaller nodes
- Clusters with limited available resources
- Existing deployments that were running with the previous lower defaults

**Performance improvement**: The new defaults are sized for production workloads and ensure RocksDB has adequate memory (<50% of container limit) for optimal performance.

#### Impact on Users

**Existing deployments upgrading the Helm chart**:
- **Pods may fail to schedule** if your nodes don't have 8Gi memory and 4 CPU cores available
- If you were relying on the previous defaults (3Gi memory limit, 1Gi request), your pods will require significantly more resources after the upgrade
- Helm upgrade will attempt to recreate pods with new resource requirements

**New deployments**:
- Will use the new production-ready defaults
- Ensure your cluster has nodes with sufficient resources (at least 8Gi memory, 4+ CPU cores)

**Custom configurations**:
- If you already specified custom `resources` in your values file, your settings will continue to be used
- If you provision Restate with a different container memory limit, update `RESTATE_ROCKSDB_TOTAL_MEMORY_SIZE` to be within ~20-50% of that limit

#### Migration Guidance

**Option 1: Accept the new defaults (recommended for production)**

Ensure your cluster has nodes with sufficient resources. The new defaults are sized for production workloads.

**Option 2: Keep previous resource limits**

If you need to maintain the previous resource limits (e.g., for development clusters), explicitly set them in your values file:

```yaml
env:
  - name: RESTATE_ROCKSDB_TOTAL_MEMORY_SIZE
    value: "1Gi"  # ~<50% of 3Gi memory limit

resources:
  limits:
    cpu: 1
    memory: 3Gi
  requests:
    cpu: 500m
    memory: 1Gi
```

**Option 3: Custom sizing**

Scale resources based on your workload. Keep RocksDB memory between 20-50% of container memory:

| Container Memory | Recommended RocksDB Memory | CPU (suggested) |
|------------------|----------------------------|-----------------|
| 4Gi | 2Gi                        | 2 |
| 8Gi (new default) | 3Gi (new default)          | 4-6 |
| 16Gi | 7Gi                        | 8 |
| 32Gi | 15Gi                       | 16 |

```yaml
env:
  - name: RESTATE_ROCKSDB_TOTAL_MEMORY_SIZE
    value: "<20-50% of memory limit>"

resources:
  limits:
    cpu: <your-cpu-limit>
    memory: <your-memory-limit>
  requests:
    cpu: <your-cpu-request>
    memory: <your-memory-request>
```

**Related Issues:**
- [#3927](https://github.com/restatedev/restate/pull/3927): Set default value of RESTATE_ROCKSDB_TOTAL_MEMORY_SIZE in helm chart
- [#3925](https://github.com/restatedev/restate/pull/3925): Warn if cgroup memory limit is misaligned with RocksDB memory limit

---

### Automatic Partition Store Memory Management

The partition store now includes an **automatic memory manager** that dynamically balances RocksDB memory budget across all active partition stores. The system monitors memory usage every 5 seconds and automatically redistributes memory when partitions are opened or closed. This ensures that the system is using the available memory more efficiently.

#### Removed Configuration Options

| Removed Option | Reason |
|----------------|--------|
| `worker.storage.num-partitions-to-share-memory-budget` | No longer needed; partition count is now determined automatically |
| `common.rocksdb-write-stall-threshold` | Write stalls disabled by default to prevent indefinite hangs |
| `common.rocksdb-enable-stall-on-memory-limit` | Write stalls disabled by default |

#### Other Changes

- Compression algorithm changed from LZ4 to ZSTD for all partition stores

#### Why This Matters

- **Simplified configuration**: No need to manually estimate the number of partitions when configuring memory budgets
- **Dynamic balancing**: Memory is automatically redistributed as partitions are opened/closed during normal operation or rebalancing
- **Better stability**: Disabling write stalls prevents indefinite hangs under memory pressure
- **Container awareness**: On Linux, the system automatically detects container memory limits (cgroup v1/v2) and warns if RocksDB memory is misconfigured

#### Impact on Users

- **Existing deployments**: If you configured `num-partitions-to-share-memory-budget`, remove it from your configuration as it is no longer needed
- **Compression**: Existing data will continue to be readable; new data will use ZSTD compression

#### Migration Guidance

**Remove deprecated options** from your configuration:
```toml
# Remove these if present:
# [worker.storage]
# num-partitions-to-share-memory-budget = ...

# [common]
# rocksdb-write-stall-threshold = ...
# rocksdb-enable-stall-on-memory-limit = ...
```

**Related Issues:**
- [#3804](https://github.com/restatedev/restate/pull/3804): Automatic partition store memory manager

---

### Journal Entry Size Limits Enforced

Restate now enforces strict size limits on journal entries (such as state values, `ctx.run()` results, and request/response payloads). When a journal entry exceeds the configured `message-size-limit`, the invocation will fail with error code **RT0003** (`MessageSizeLimit`) and be retried according to the configured retry policy, rather than causing system instability.

The default limit is **32 MiB** (from `networking.message-size-limit`).

#### Why This Matters

Previously, when services created journal entries larger than the internal message size limit (32 MiB), the system could enter an unrecoverable state:

- The `SequencerAppender` would retry indefinitely trying to replicate messages that would never succeed
- This blocked all subsequent appends to the log
- Eventually, the entire cluster could become unresponsive

This was particularly problematic in distributed clusters where log replication between nodes failed due to network message size constraints, while local in-memory replication (which has no size limit) would succeed.

#### Impact on Users

- **Services with large state values**: If your services store state entries larger than 32 MiB, these operations will now fail with error code `RT0003` (`MessageSizeLimit`) instead of causing cluster instability
- **Large `ctx.run()` results**: Side effect results exceeding the limit will cause the invocation to fail
- **Large request/response payloads**: Payloads exceeding the limit will be rejected

The invocation will fail and be retried according to the configured retry policy. If the payload size issue is not resolved (e.g., by updating the service code), the invocation will eventually be paused or killed after exhausting retries.

#### Configuration

The limit is controlled by `networking.message-size-limit` (default: 32 MiB). Component-specific limits are automatically clamped to this value:

```toml
[networking]
message-size-limit = "32MiB"  # default

# Component-specific limits (optional, clamped to networking limit)
[worker.invoker]
message-size-limit = "32MiB"

[bifrost]
# record-size-limit is derived from networking.message-size-limit
```

#### Migration Guidance

1. **Review your payload sizes**: If you have services that store large state values or return large results, ensure they stay under 32 MiB per entry

2. **Split large data**: Instead of storing one large state entry, consider:
   - Splitting data across multiple state keys
   - Using external storage for large blobs and storing references in Restate state
   - Chunking large results into smaller pieces

3. **Monitor for errors**: After upgrading, watch for `RT0003` error codes which indicate message size limit exceeded

4. **Increase limit if needed**: If 32 MiB is insufficient, increase the limit:
   ```toml
   [networking]
   message-size-limit = "64MiB"
   ```
   Note: Increasing this limit increases memory pressure on the cluster during replication.

**Related Issues:**
- [#4129](https://github.com/restatedev/restate/issues/4129): Improve handling of large messages
- [#4132](https://github.com/restatedev/restate/issues/4132): Make Bifrost batching aware of message size limits
- [#4137](https://github.com/restatedev/restate/pull/4137): Unify message_size_limit configuration across gRPC services

---

### gRPC Message Size Configuration Renamed

1. **Configuration key renamed**: `networking.max-message-size` is now `networking.message-size-limit`
2. **New default**: The default message size limit is now **32 MiB** (previously varied by component)
3. **Unified configuration**: A single setting now controls message size limits across all gRPC services

**Renamed configuration options:**

| Old Name | New Name |
|----------|----------|
| `networking.max-message-size` | `networking.message-size-limit` |
| `common.metadata-client.max-message-size` | `common.metadata-client.message-size-limit` |

#### Why This Matters

- **Larger payloads supported**: The 32 MiB default allows for larger state entries, `ctx.run()` results, and request/response payloads
- **Simplified configuration**: One setting applies uniformly to all cluster communication
- **Consistency**: Sub-limits (`worker.invoker.message-size-limit`, `common.metadata-client.message-size-limit`) are automatically clamped to not exceed the networking limit

#### Impact on Users

- **Configuration migration**: If you explicitly configured `networking.max-message-size`, rename it
- **Larger limits**: If you previously hit message size limits, the new 32 MiB default may resolve the issue

#### Migration Guidance

**Before:**
```toml
[networking]
max-message-size = "16MiB"

[common.metadata-client]
max-message-size = "16MiB"
```

**After:**
```toml
[networking]
message-size-limit = "16MiB"

[common.metadata-client]
message-size-limit = "16MiB"
```

**When to increase the limit**: Consider increasing `networking.message-size-limit` if you have:
- Large virtual object state entries (> 32 MiB)
- Large `ctx.run()` results
- Large request/response payloads

**Related Issues:**
- [#4100](https://github.com/restatedev/restate/pull/4100): Configurable gRPC max encoding/decoding message size
- [#4137](https://github.com/restatedev/restate/pull/4137): Unify message_size_limit configuration across gRPC services

---

### Prometheus Metrics Naming Convention Fixes

Several Prometheus metric names have been corrected to follow proper naming conventions:

1. **RocksDB gauge metrics no longer have incorrect `_count` suffix**:
   - `restate_rocksdb_actual_delayed_write_rate_count` → `restate_rocksdb_actual_delayed_write_rate`
   - `restate_rocksdb_background_errors_count` → `restate_rocksdb_background_errors`
   - `restate_rocksdb_compaction_pending_count` → `restate_rocksdb_compaction_pending`
   - `restate_rocksdb_estimate_num_keys_count` → `restate_rocksdb_estimate_num_keys`
   - `restate_rocksdb_is_write_stopped_count` → `restate_rocksdb_is_write_stopped`
   - `restate_rocksdb_mem_table_flush_pending_count` → `restate_rocksdb_mem_table_flush_pending`
   - `restate_rocksdb_min_log_number_to_keep_count` → `restate_rocksdb_min_log_number_to_keep`
   - `restate_rocksdb_num_deletes_active_mem_table_count` → `restate_rocksdb_num_deletes_active_mem_table`
   - `restate_rocksdb_num_deletes_imm_mem_tables_count` → `restate_rocksdb_num_deletes_imm_mem_tables`
   - `restate_rocksdb_num_entries_active_mem_table_count` → `restate_rocksdb_num_entries_active_mem_table`
   - `restate_rocksdb_num_entries_imm_mem_tables_count` → `restate_rocksdb_num_entries_imm_mem_tables`
   - `restate_rocksdb_num_files_at_level{0-6}_count` → `restate_rocksdb_num_files_at_level{0-6}`
   - `restate_rocksdb_num_immutable_mem_table_count` → `restate_rocksdb_num_immutable_mem_table`
   - `restate_rocksdb_num_live_versions_count` → `restate_rocksdb_num_live_versions`
   - `restate_rocksdb_num_running_compactions_count` → `restate_rocksdb_num_running_compactions`
   - `restate_rocksdb_num_running_flushes_count` → `restate_rocksdb_num_running_flushes`

2. **Fixed duplicate `_bytes` suffix**:
   - `restate_rocksdb_estimate_pending_compaction_bytes_bytes` → `restate_rocksdb_estimate_pending_compaction_bytes`

3. **Fixed duplicate `_count` suffix on summary metrics**:
   - `restate_rocksdb_num_sst_read_per_level_count` → `restate_rocksdb_num_sst_read_per_level`
   - `restate_rocksdb_read_num_merge_operands_count` → `restate_rocksdb_read_num_merge_operands`

#### Why This Matters

The previous metric names violated Prometheus naming conventions:
- The `_count` suffix should only be used for the count component of summary/histogram metrics, not for gauges that measure quantities
- Having `_bytes_bytes` or `_count_count` suffixes was incorrect and confusing

The new names follow [Prometheus best practices for metric naming](https://prometheus.io/docs/practices/naming/).

#### Impact on Users

- **Prometheus/Grafana dashboards**: Any dashboards, alerts, or recording rules that reference the old metric names will need to be updated
- **Monitoring integrations**: External monitoring systems querying these metrics will need updates

#### Migration Guidance

Update any Prometheus queries, Grafana dashboards, or alerting rules to use the new metric names. For example:

```promql
# Old query
sum(restate_rocksdb_background_errors_count)

# New query
sum(restate_rocksdb_background_errors)
```

If you have many dashboards to update, you can use find-and-replace to remove the `_count` suffix from RocksDB gauge metrics:
- Replace `_count{` with `{` for affected metrics
- Replace `_bytes_bytes` with `_bytes` for `estimate_pending_compaction_bytes`

---

## Deprecations

### /cluster-health Endpoint Deprecated

The `/cluster-health` endpoint on the admin API (port 9070) is now deprecated and will be removed in v1.7.0.

#### Why This Matters

This endpoint was unintentionally exposed publicly and is no longer used by Restate internally. It does not provide meaningful health information for external monitoring purposes.

#### Impact on Users

- **If you use `/cluster-health`**: Stop using this endpoint before upgrading to v1.7.0
- **Health checks**: Use the `/health` endpoint on the ingress port (8080) or admin port (9070) instead for liveness/readiness probes

#### Migration Guidance

Replace any usage of `/cluster-health` with appropriate alternatives:

```bash
# Old (deprecated, will be removed in v1.7.0)
curl http://localhost:9070/cluster-health

# New alternatives for health checks
curl http://localhost:9070/health  # Admin API health
curl http://localhost:8080/restate/health  # Ingress health
```

**Related Issues:**
- [#3898](https://github.com/restatedev/restate/issues/3898): Deprecate cluster-health endpoint

---

## New Features

### Manual Pause and Resume for Invocations

You can now manually **pause** running invocations and **resume** them later, optionally on a different deployment. This provides explicit control over invocation execution independent of the automatic retry mechanism.

#### Why This Matters

- **Debugging**: Pause invocations stuck in retry loops to investigate the root cause without losing progress
- **Managed deployments**: Pause and resume capabilities integrate with deployment flows. See the [deployment flows documentation](https://docs.restate.dev/services/deployment/overview) for details.
- **Resource management**: Temporarily stop invocations consuming resources during maintenance
- **Investigation**: Pause to examine journal state before deciding to resume, restart-as-new, or kill

#### Impact on Users

- **New CLI commands**: `restate invocations pause` and `restate invocations resume`
- **New API endpoints**: `PATCH /invocations/{id}/pause` and `PATCH /invocations/{id}/resume`
- **Default behavior**: By default, invocations are now paused (not killed) when `max-attempts` is exhausted. This can be configured via `on-max-attempts` in the retry policy.

#### Usage

**UI:**

Pause and resume invocations directly from the Restate UI invocation detail page. See the [managing invocations documentation](https://docs.restate.dev/services/invocation/managing-invocations#pause) for more details.

**CLI:**
```bash
# Pause a running invocation
restate invocations pause <invocation_id>

# Pause all running invocations matching a query
restate invocations pause MyService/myHandler

# Resume a paused invocation
restate invocations resume <invocation_id>

# Resume on a different deployment
restate invocations resume <invocation_id> --deployment latest
restate invocations resume <invocation_id> --deployment <deployment_id>
```

**REST API:**

For REST API details, see the Admin API documentation for [resuming](https://docs.restate.dev/admin-api/invocation/resume-an-invocation) and [pausing](https://docs.restate.dev/admin-api/invocation/pause-an-invocation) invocations.

**Resume deployment options:**
- `latest`: Use the latest deployment for the service
- `keep`: Keep the currently pinned deployment
- `<deployment_id>`: Use a specific deployment ID

#### Behavior Details

When an invocation is **paused**:
- The current execution attempt is gracefully terminated
- Any pending retry timer is cancelled
- The invocation will **not** retry or execute until explicitly resumed

When an invocation is **resumed**:
- Execution restarts from the last journal checkpoint
- If a deployment is specified, the invocation is redirected to that deployment

**Constraints:**
- Only `running` or `backing-off` invocations can be paused
- Paused and suspended invocations can be resumed
- If a cancel signal arrives while paused, the invocation automatically resumes to process the cancellation

**Related Issues:**
- [#3676](https://github.com/restatedev/restate/pull/3676): Pause and Resume
- [#3881](https://github.com/restatedev/restate/pull/3881): Manual pause an invocation
- [#3948](https://github.com/restatedev/restate/pull/3948): Add --deployment option to invocation resume CLI command

---

### DynamoDB Metadata Store Provider

Restate now supports using **Amazon DynamoDB** as a metadata store provider, as an alternative to the built-in Raft-based replicated metadata server.

#### Why This Matters

- **Simplified operations**: No need to manage Raft consensus for metadata; DynamoDB handles replication and durability
- **AWS-native integration**: Leverage DynamoDB's managed infrastructure, automatic scaling, and high availability
- **Multi-cluster support**: Share a single DynamoDB table across multiple Restate clusters using key prefixes

#### Impact on Users

- **New optional feature**: Existing deployments are unaffected; this is an opt-in alternative to the replicated metadata server
- **Manual table creation required**: The DynamoDB table must be created before starting Restate

#### Configuration

**1. Create the DynamoDB table:**

```bash
aws dynamodb create-table \
    --table-name metadata \
    --attribute-definitions AttributeName=pk,AttributeType=S \
    --key-schema AttributeName=pk,KeyType=HASH \
    --billing-mode PAY_PER_REQUEST
```

The table must have a string partition key named `pk`. Choose an appropriate table name and billing mode for your setup.

**2. Configure Restate to use DynamoDB:**

```toml
roles = [
    "http-ingress",
    "admin",
    "worker",
    "log-server",
]
# Note: "metadata-server" role is removed

[metadata-client]
type = "dynamo-db"
table = "metadata"

# Authentication: use EITHER aws-profile OR explicit credentials
aws-profile = "my-profile"
# aws-access-key-id = "..."
# aws-secret-access-key = "..."
# aws-session-token = "..."        # For STS temporary credentials

# Optional settings
aws-region = "eu-central-1"        # Required unless inferred from profile/environment
# key-prefix = "prod-cluster/"     # Namespace for multi-cluster setups
# aws-endpoint-url = "http://localhost:8000"  # For local testing
```

**Required IAM permissions:**
- `dynamodb:GetItem`
- `dynamodb:PutItem`
- `dynamodb:DeleteItem`

**Related Issues:**
- [#3551](https://github.com/restatedev/restate/issues/3551): Add DynamoDB-based metadata store integration

---

### Restart Invocation from Journal Prefix

The `restart-as-new` operation now supports restarting a completed invocation from a specific journal entry index, preserving partial progress from the original invocation.

#### Why This Matters

- **Preserve expensive work**: When restarting an invocation, you can keep completed side effects (calls, sleeps, runs) instead of re-executing everything
- **Managed deployments**: Restart-as-new integrates with deployment flows. See the [deployment flows documentation](https://docs.restate.dev/services/deployment/overview) for details.
- **Recovery**: Recover an invocation that recorded a journal entry leading to unwanted results (e.g., poison pill, invalid token) while preserving the work from before

#### Impact on Users

- **New API parameter**: The `restart-as-new` endpoint now accepts a `from` parameter to specify the journal entry index
- **Protocol requirement**: Restarting from `from > 0` requires **Service Protocol V6** or later

#### Usage

**UI:**

Restart invocations as new directly from the Restate UI invocation detail page. For prefix-based restarts with a specific journal entry, use the UI or REST API.

**CLI:**
```bash
# Restart a completed invocation (from beginning)
restate invocations restart-as-new <invocation_id>
```

Note: The CLI currently restarts from the beginning (`from=0`). Use the UI or REST API for prefix-based restarts.

**REST API:**

For REST API details, see the [Admin API documentation](https://docs.restate.dev/admin-api/invocation/restart-as-new-invocation).

#### Behavior Details

When restarting from a prefix:
1. Journal entries from index 0 to `from` (inclusive) are copied to the new invocation
2. The new invocation receives a new invocation ID and resumes from where the prefix ends

**Requirements:**
- The original invocation must be **completed** (not running, scheduled, or in inbox)
- **Workflows** can only be restarted as new and not with a prefix
- Restarting from `from > 0` requires Protocol V6+; older protocols only support `from=0`

**Related Issues:**
- [#3761](https://github.com/restatedev/restate/pull/3761): Restart as new from prefix
- [#3885](https://github.com/restatedev/restate/pull/3885): Restart as new changes for 1.6

---

### Deployment Registration and Management Improvements

This release introduces significant improvements to deployment registration and management, providing better safety, flexibility, and observability.

#### Idempotent Deployment Registration

Re-registering the same deployment without changes now returns HTTP 200 instead of an error, making deployment scripts and CI/CD pipelines more robust.

#### New `breaking` Flag

A new `breaking` flag has been added to deployment registration. Use this when you need to allow breaking changes (e.g., changing a service type) while still protecting against accidental overwrites.

#### Default for `force` Changed to `false`

The `force` parameter for **deployment registration** now defaults to `false`. This provides better protection against accidentally overwriting deployments in production.

#### Deployment Metadata

You can now attach arbitrary metadata to deployments during registration for improved observability. When deploying from GitHub Actions, Restate automatically captures environment information (repository, commit, workflow) to link deployments back to their source.

#### Update Deployment API

A new `PATCH /deployments/{id}` endpoint allows updating deployment settings without re-registering:
- **Update headers**: Fix expired tokens or rotate credentials without re-deploying
- **Update assume_role_arn**: Change AWS IAM role configuration for Lambda deployments
- **Update endpoint address**: Redirect invocations to a different URL

#### Why This Matters

- **Safer deployments**: Default `force=false` prevents accidental overwrites
- **Better CI/CD integration**: Idempotent registration simplifies deployment scripts
- **Improved observability**: Deployment metadata links back to source control
- **Operational flexibility**: Update credentials and configuration without service disruption

#### Impact on Users

- **API version**: These changes are part of Admin API V3. The CLI automatically uses V3.
- **Existing scripts**: Scripts using `force=true` explicitly will continue to work unchanged
- **New scripts**: Scripts relying on the previous `force=true` default should be updated to explicitly pass `--force` if needed

#### Usage

For detailed usage examples and deployment workflows, see the [deployment flows documentation](https://docs.restate.dev/services/deployment/overview).

**CLI - Register with breaking changes allowed:**
```bash
restate deployments register http://localhost:9080 --breaking
```

**CLI - Force overwrite:**
```bash
restate deployments register http://localhost:9080 --force
```

**Update deployment headers:**
```bash
curl -X PATCH "http://localhost:9070/deployments/{id}" \
  -H "Content-Type: application/json" \
  -d '{"additional_headers": {"Authorization": "Bearer new-token"}}'
```

**Related Issues:**
- [#3859](https://github.com/restatedev/restate/pull/3859): Deployments improvements
- [#3144](https://github.com/restatedev/restate/issues/3144): Service deployment / registration issues

---

### Partition-Driven Log Trimming

Log trimming is now handled directly by partition leaders rather than by a centralized cluster controller task. This provides more responsive trimming behavior - logs are trimmed immediately when durability conditions are met, rather than on an hourly schedule.

**Removed configuration options:**

| Removed Option | Previous Default |
|----------------|------------------|
| `admin.log-trim-check-interval` | 1 hour |
| `worker.experimental-partition-driven-log-trimming` | false |

**New configuration options:**

| New Option | Type | Default | Description |
|------------|------|---------|-------------|
| `worker.durability-mode` | string | `balanced` (with snapshots) / `replica-set-only` (without) | Controls when partition store state is considered durable enough to trim the log |
| `worker.trim-delay-interval` | duration | `0s` | Delay trimming after durability condition is met (useful for geo-replicated S3) |

**Durability Modes:**

The `worker.durability-mode` option controls the criteria for determining when it's safe to trim the log:

| Mode | Description |
|------|-------------|
| `balanced` | Partition store is durable when covered by a snapshot **and** at least one replica has flushed to local storage. **Default when a snapshot repository is configured.** |
| `snapshot-only` | Partition store is durable only after a snapshot has been created, regardless of local replica state. |
| `snapshot-and-replica-set` | Partition store is durable when **all** replicas have flushed locally **and** a snapshot exists. |
| `replica-set-only` | Partition store is durable when all replicas have flushed locally, regardless of snapshot state. **Default when no snapshot repository is configured.** |
| `none` | Disables automatic durability tracking and trimming entirely. |

See the [documentation](https://docs.restate.dev/server/snapshots#log-trimming-and-durability) for more details on log trimming and durability.

#### Why This Matters

- **More responsive**: Logs are trimmed as soon as they are safe to remove, reducing storage usage more quickly
- **Simpler operation**: No need to configure or tune trim check intervals
- **Better scalability**: Each partition leader independently manages its own log trimming
- **Flexible durability**: New durability modes allow fine-grained control over when logs can be trimmed

#### Impact on Users

- **Configuration cleanup**: Remove `admin.log-trim-check-interval` if you had it configured
- **Requirements unchanged**: Multi-node clusters still require a configured snapshot repository (`worker.snapshots.destination`) for automatic log trimming. Without it, automatic trimming is disabled and a warning is logged.
- **Single-node deployments**: Can trim based on local durability without snapshots

#### Migration Guidance

1. **Remove deprecated options** from your configuration if present:
   ```toml
   # Remove these if present:
   # [admin]
   # log-trim-check-interval = "1h"
   
   # [worker]
   # experimental-partition-driven-log-trimming = true
   ```

2. **Ensure snapshot repository is configured** for multi-node clusters:
   ```toml
   [worker.snapshots]
   destination = "s3://your-bucket/snapshots"
   ```

3. **(Optional) Configure durability mode and trim delay** if you need non-default behavior:
   ```toml
   [worker]
   # Controls when partition store state is considered durable enough to trim the log
   # Values: "balanced", "snapshot-only", "snapshot-and-replica-set", "replica-set-only", "none"
   durability-mode = "balanced"
   
   # Delay trimming after durability condition is met (useful for geo-replicated S3)
   trim-delay-interval = "5m"
   ```

**Related Issues:**
- [#3479](https://github.com/restatedev/restate/issues/3479) Partition-driven log trimming
- [#3842](https://github.com/restatedev/restate/pull/3842): Partition-driven trimming is now the default

---

### Batch Invocation Operations

Added support for batch invocation operations to efficiently manage multiple invocations at once, with improvements to both the UI and CLI.

#### Why This Matters

- **Efficient management**: Operate on hundreds of invocations without manual iteration
- **Better UX**: Progress bars and concurrency limits provide visibility and prevent overwhelming the server
- **UI support**: The web UI can now perform batch operations efficiently

#### Impact on Users

- **CLI users**: Batch commands now show progress and execute faster
- **UI users**: Can perform batch operations directly from the web interface

#### Usage

**CLI batch operations:**
```bash
# Cancel all running invocations for a service (with progress bar)
restate invocations cancel MyService

# Kill invocations with a limit
restate invocations kill MyService --limit 100

# Purge completed invocations
restate invocations purge MyService/myHandler

# Pause all running invocations matching a query
restate invocations pause MyService

# Resume all paused invocations
restate invocations resume MyService

# Restart completed invocations as new
restate invocations restart-as-new MyService
```

**Query patterns supported:**
- Service name: `MyService`
- Service and handler: `MyService/myHandler`
- Virtual object with key: `MyVirtualObject/myKey`
- Specific invocation ID: `inv_1abc...`

**Options:**
- `--limit <N>`: Maximum number of invocations to process (default: 500)
- `--yes` / `-y`: Skip confirmation prompt

**Related Issues:**
- [#3856](https://github.com/restatedev/restate/issues/3856): Add support for bulk operations

---

### Network Configuration Overhaul

This release introduces a major rework of how network ports and addresses are configured in Restate Server. The changes bring new capabilities, simplified configuration, and better defaults.

Additionally, `bind-address` is no longer derived from `advertised-address`. These are now independent configuration options.

#### Unix Domain Sockets Support

Restate Server now supports listening on Unix domain sockets for all services (fabric, admin, ingress, and tokio-console). By default, the server listens on **both** TCP sockets and Unix domain sockets simultaneously. Unix sockets are created under the `restate-data/` directory:
- `restate-data/fabric.sock` - Node-to-node communication
- `restate-data/admin.sock` - Admin API
- `restate-data/ingress.sock` - HTTP ingress
- `restate-data/tokio.sock` - Tokio console (if enabled)

#### Random Ports

You can now easily start `restate-server` binding to random ports using the command line argument `--use-random-ports=true`. For example:

```shell
$ restate-server --use-random-ports=true --no-logo
Admin: http://192.168.1.17:39623/
HTTP Ingress: http://192.168.1.17:43759/
Message Fabric: http://192.168.1.17:33711/
```

The chosen ports will be displayed in the first three standard output lines.

#### New Configuration Options

The following new global options are available and can be overridden per-service:

| Option | Description | Default |
|--------|-------------|---------|
| `listen-mode` | Controls socket types: `tcp`, `unix`, or `all` | `all` |
| `use-random-ports` | Use OS-assigned random ports | `false` |
| `bind-ip` | Local interface IP address to bind | `0.0.0.0` |
| `bind-port` | Network port to listen on | Service-specific |
| `advertised-host` | Hostname for advertised addresses | Auto-detected |
| `advertised-address` | Full URL for service advertisement | Auto-derived |

#### Automatic Advertised Address Detection

The server now automatically detects a reasonable advertised address:
- When `listen-mode=unix`, advertised addresses use `unix:/` format
- When `listen-mode=tcp` or `all`, the server detects the public routable IP address instead of defaulting to `127.0.0.1`

This significantly improves Docker deployments and multi-node clusters by reducing required configuration.

#### Simplified Metadata Client Configuration

Nodes no longer need to include their own address in `metadata-client.addresses`. For single-node setups, this field can be left empty. The server automatically includes its own address if running a metadata server role.

#### Deprecated Options

The following options are deprecated and will be removed in a future release:

| Deprecated Option | Replacement |
|-------------------|-------------|
| `admin.advertised-admin-endpoint` | `admin.advertised-address` |
| `ingress.advertised-ingress-endpoint` | `ingress.advertised-address` |

#### Why This Matters

1. **Easier development**: Unit tests and local development now use Unix sockets by default, eliminating port conflicts
2. **Better container support**: Auto-detection of public IP addresses reduces configuration burden in containerized deployments
3. **Enhanced flexibility**: Socket activation support enables zero-downtime restarts via systemd or similar tools
4. **Early failure detection**: The server now binds all required ports early in startup, before opening databases

#### Impact on Users

- **Existing deployments**: Existing configurations continue to work. The server will additionally listen on Unix sockets unless `listen-mode=tcp` is set
- **New deployments**: Benefit from simplified configuration with auto-detection of advertised addresses
- **CLI tools**: Both `restate` and `restatectl` now support Unix socket connections via `unix:` prefix (e.g., `restatectl -s unix:restate-data/admin.sock status`)
- **Docker users**: Should see improved out-of-box experience with auto-detected advertised addresses

#### Migration Guidance

No migration is required. To opt out of Unix socket listening:

```toml
# Only listen on TCP sockets
listen-mode = "tcp"
```

To use only Unix sockets (useful for embedded scenarios):

```toml
# Only listen on Unix sockets
listen-mode = "unix"
```

To use random ports (useful for testing):

```bash
restate-server --use-random-ports=true
```

To migrate from deprecated options:

```toml
# Old (deprecated)
[admin]
advertised-admin-endpoint = "http://my-host:9070"

# New
[admin]
advertised-address = "http://my-host:9070"
# Or just set the host part:
advertised-host = "my-host"
```

#### Socket Activation Support

Restate now supports systemd-compatible socket activation. A parent process can pre-bind TCP or Unix socket listeners and pass them to restate-server via `LISTEN_FD`. This enables:
- Zero-downtime restarts (clients don't see connection errors during upgrades)
- Pre-allocated ports for test harnesses

Example with `systemfd`:
```bash
systemfd --no-pid -s http::9000 -- restate-server
```

**Related Issues:**
- [#3892](https://github.com/restatedev/restate/pull/3892): On listeners, addresses, and ports

---

### restate up Command

A new `restate up` command (alias: `restate dev`) is now available in the CLI for quickly starting a local Restate instance for development and testing.

#### Why This Matters

- **Quick start**: Spin up a local Restate server with a single command
- **Zero configuration**: Uses sensible defaults with ephemeral storage
- **Developer-friendly**: Opens the Admin UI automatically in your browser

#### Impact on Users

- **New CLI command**: Available in npm packages, Homebrew, and cargo-dist releases
- **Not in Docker images**: This command is intentionally excluded from Docker images (use `restate-server` for containerized deployments)

#### Usage

```bash
# Start with default settings
restate up

# Or use the alias
restate dev

# Use random ports to avoid conflicts
restate up --use-random-ports

# Use Unix sockets only
restate up --use-unix-sockets

# Keep the data directory after exit (for debugging)
restate up --retain
```

**On startup, you'll see:**
- Restate logo
- Bound addresses (Admin URL, HTTP Ingress URL)
- Data directory path
- Browser opens to Admin UI automatically

**Options:**
- `-r, --use-random-ports`: Start on random ports to avoid conflicts
- `-u, --use-unix-sockets`: Start on Unix sockets only
- `--retain`: Do not delete the temporary data directory after exiting

#### Important Notes

> **This tool is for local development and testing only.**
>
> Do NOT use `restate up` in production. For production deployments, use `restate-server` with proper configuration.

- Data is stored in an **ephemeral temporary directory** that is deleted on exit (unless `--retain` is used)
- Single node, single partition - no replication
- Press Ctrl+C to stop

**Related Issues:**
- [#3904](https://github.com/restatedev/restate/pull/3904): restate up command

---

### Kafka SASL OAUTHBEARER/OIDC Authentication

The Kafka ingress now supports **SASL OAUTHBEARER** authentication, enabling OAuth 2.0/OpenID Connect token-based connections to managed Kafka services. This feature is enabled by default in all restate-server builds.

#### Why This Matters

- **Managed Kafka services**: Connect to Confluent Cloud, Amazon MSK (with IAM authentication), Azure Event Hubs, and other providers that require OAuth-based authentication
- **Enhanced security**: Use short-lived OAuth tokens instead of long-lived credentials
- **Standard authentication**: OAUTHBEARER is the industry-standard mechanism for modern Kafka deployments

#### Impact on Users

- **New capability**: Users can now configure Kafka subscriptions with OAUTHBEARER authentication
- **Enabled by default**: No special build flags needed; the feature is included in standard restate-server binaries

#### Configuration

Configure SASL OAUTHBEARER via the `additional_options` field in your Kafka cluster configuration. These options are passed directly to librdkafka.

**Example for Confluent Cloud:**
```toml
[[ingress.kafka-clusters]]
name = "my-cluster"
brokers = ["SASL_SSL://pkc-xxxxxx.eu-central-1.aws.confluent.cloud:9092"]
"security.protocol"="SASL_SSL"
"sasl.mechanisms"="OAUTHBEARER"
"sasl.oauthbearer.method"="oidc"
"sasl.oauthbearer.client.id"="<your-client-id>"
"sasl.oauthbearer.client.secret"="<your-client-secret>"
"sasl.oauthbearer.token.endpoint.url"="<your-token-endpoint>"
"sasl.oauthbearer.scope"="kafka"
```

**Common OAUTHBEARER options:**

| Option | Description |
|--------|-------------|
| `security.protocol` | Set to `SASL_SSL` for encrypted connections |
| `sasl.mechanism` | Set to `OAUTHBEARER` |
| `sasl.oauthbearer.method` | Set to `oidc` for OIDC-based token retrieval |
| `sasl.oauthbearer.client.id` | OAuth client ID |
| `sasl.oauthbearer.client.secret` | OAuth client secret |
| `sasl.oauthbearer.token.endpoint.url` | OAuth token endpoint URL |
| `sasl.oauthbearer.scope` | OAuth scope (if required by provider) |

For the full list of available options, see the [librdkafka CONFIGURATION.md](https://github.com/confluentinc/librdkafka/blob/master/CONFIGURATION.md).

**Related Issues:**
- [#3995](https://github.com/restatedev/restate/issues/3995): Introduce kafka-oidc feature and enable it by default

---

### Error Events Enabled by Default

Error events are now enabled by default. When invocations encounter transient failures, Restate automatically records detailed error information in the journal, accessible via the `sys_journal_events` SQL table.

#### Why This Matters

- **Better observability**: See exactly why invocations are failing during retries
- **Debugging**: Access error codes, messages, stacktraces, and the related command that caused the failure

#### Impact on Users

- **Improved observability in the invocation timeline UI**: Error events are now visible in the Restate UI invocation timeline
- **New SQL table**: `sys_journal_events` is now populated with error events
- **No configuration needed**: Error events are recorded automatically
- **Removed config**: The experimental flag `worker.invoker.experimental_features_propose_events` has been removed

#### Usage

Query error events via SQL:
```sql
SELECT * FROM sys_journal_events WHERE id = '<invocation-id>'
```

**Table columns:**
- `id`: The invocation ID
- `after_journal_entry_index`: Journal index after which the event occurred
- `appended_at`: Timestamp when the event was recorded
- `event_type`: Either `TransientError` or `Paused`
- `event_json`: Full event details as JSON

**Event types:**

1. **TransientError**: Recorded when an invocation fails with a transient error and will be retried
   - Includes: error code, message, stacktrace (if available), related command info

2. **Paused**: Recorded when an invocation is paused (manually or due to max retries)
   - Includes: last failure information if the pause was due to errors

**Example event_json for TransientError:**
```json
{
  "error_code": 500,
  "error_message": "Connection refused",
  "error_stacktrace": "...",
  "restate_doc_error_code": "RT0001",
  "related_command_index": 3,
  "related_command_name": "Call",
  "related_command_type": "Call"
}
```

**Related Issues:**
- [#3301](https://github.com/restatedev/restate/pull/3301): Store transient errors in journal events
- [#3843](https://github.com/restatedev/restate/pull/3843): Enable error events

---

### Snapshot Improvements

Two new capabilities for partition snapshots:

1. **Time-based snapshot intervals**: Configure automatic snapshots based on elapsed wall-clock time, in addition to the existing record-count trigger.

2. **Azure Blob Storage and Google Cloud Storage support**: Snapshots can now be stored in Azure Blob Storage or GCS, in addition to Amazon S3.

#### Why This Matters

- **Flexible scheduling**: Time-based intervals ensure snapshots happen regularly even during low-activity periods
- **Multi-cloud support**: Use native cloud storage without needing S3-compatible proxy services
- **Simplified operations**: Native authentication with Azure AD or GCP service accounts

#### Impact on Users

- **New configuration option**: `worker.snapshots.snapshot-interval` for time-based triggers
- **New storage destinations**: `az://` / `azure://` for Azure, `gs://` for GCS
- **Breaking**: The undocumented `file://` protocol for snapshot destinations has been removed

#### Configuration

**Time-based snapshot intervals:**
```toml
[worker.snapshots]
destination = "s3://my-bucket/snapshots"

# Time-based interval (new)
snapshot-interval = "1h"

# Record-based interval (existing)
snapshot-interval-num-records = 100000
```

When both options are configured, **both conditions must be satisfied** before a snapshot is created. This prevents excessive snapshots during low-activity periods.

**Azure Blob Storage:**
```toml
[worker.snapshots]
destination = "az://my-container/snapshots"
# or
destination = "azure://my-container/snapshots"
```

Configure authentication via environment variables:
- `AZURE_STORAGE_ACCOUNT_NAME` and `AZURE_STORAGE_ACCOUNT_KEY`
- Or use Azure AD / managed identity

**Google Cloud Storage:**
```toml
[worker.snapshots]
destination = "gs://my-bucket/snapshots"
```

Configure authentication via environment variables:
- `GOOGLE_SERVICE_ACCOUNT` (path to service account JSON)
- Or use Application Default Credentials (ADC)

**S3 (existing):**
```toml
[worker.snapshots]
destination = "s3://my-bucket/snapshots"
```

#### Migration Guidance

- **Existing S3 users**: No changes required
- **Azure/GCP users**: Configure the appropriate destination URL and authentication environment variables
- **`file://` users**: The `file://` protocol is no longer supported. Use S3-compatible local services like MinIO for testing.

**Related Issues:**
- [#3918](https://github.com/restatedev/restate/pull/3918): Add support for time-based snapshot intervals
- [#3906](https://github.com/restatedev/restate/pull/3906): Enable Azure and GCS object-store protocols for partition snapshots

---

### Snapshot Retention (Experimental)

Restate can now be configured to retain a fixed number of recent snapshots and automatically delete older ones. Previously, snapshot cleanup required external lifecycle policies or manual intervention.

New configuration option:

```toml
[worker.snapshots]
experimental-num-retained = 10  # retain the 10 most recent snapshots
```

#### Why This Matters

This feature:

- Eliminates the need for external cleanup scripts or object store lifecycle policies
- Ensures all retained snapshots are usable for partition recovery

#### Behavioral Change: Archived LSN Semantics

**Important**: When `experimental-num-retained` is configured, the reported **Archived LSN** changes meaning:

| Configuration                     | Archived LSN means                      |
| --------------------------------- | --------------------------------------- |
| No retention configured (default) | LSN of the latest snapshot              |
| `experimental-num-retained = N`   | LSN of the **oldest** retained snapshot |

This change ensures that any retained snapshot can be used for recovery. This it also means that:

- The log trim point will be further from the tail, retaining more records
- Log storage usage on log-server nodes will increase proportionally to the number of retained snapshots and activity in the cluster / snapshot interval

#### Impact on Users

- **Existing deployments**: No change unless you enable `experimental-num-retained`
- **New deployments**: Opt-in feature, disabled by default
- **Enabling retention**: Updates the partition `latest.json` metadata format from V1 to V2

#### Migration Guidance

- **Enabling**: Simply add `experimental-num-retained` to your configuration
- **Downgrading**: Not recommended once V2 format is written; older Restate versions will not recognize the retained snapshot tracking and will revert to V1 behavior
- **Disk planning**: Account for additional log retention when setting `num-retained` higher than 1

**Related Issues:**
- [#3942](https://github.com/restatedev/restate/pull/3942): Add support for managing a fixed number of retained snapshots

---

### Kafka Batch Ingestion (Experimental)

A new experimental ingestion client provides more efficient invocation ingestion, particularly beneficial for Kafka subscriptions. When enabled, invocations are batched and routed through the Partition Processor, which becomes the sole writer to its logs.

#### Why This Matters

In certain scenarios, such as consuming from a Kafka topic with few partitions, the new ingestion client can significantly improve ingestion throughput compared to the legacy implementation.

#### Impact on Users

- **Experimental**: This feature is disabled by default and should be used with caution
- **Cluster-wide**: All nodes in the cluster must be running v1.6 before enabling this feature
- **No rollback**: Once enabled and data has been ingested, you cannot roll back to a version prior to v1.6

#### Usage

To enable the experimental Kafka batch ingestion, set the following environment variable on all nodes:

```bash
RESTATE_EXPERIMENTAL_KAFKA_BATCH_INGESTION=true
```

Or in your configuration file:

```toml
experimental-kafka-batch-ingestion = true
```

**Important**: Ensure all nodes in your cluster are running v1.6 before enabling this feature.

**Related Issues:**
- [#3976](https://github.com/restatedev/restate/pull/3976): Ingestion client crate
- [#3975](https://github.com/restatedev/restate/pull/3975): Kafka ingress refactor to use ingestion-client

---

## Improvements

### Behavioral Changes

#### RocksDB Memory Defaults Changed

The default RocksDB memory configuration has been changed:
- **`rocksdb-total-memory-size`**: Changed from **6 GiB** to **2 GiB**
- **`rocksdb-total-memtables-ratio`**: Changed from **0.5 (50%)** to **0.85 (85%)**

This means the default block cache size is now approximately 300 MiB (15% of 2 GiB) compared to the previous 3 GiB (50% of 6 GiB).

**Why This Matters:**

These changes address two goals:

1. **Reduced memory confusion**: The previous 6 GiB default caused Restate's memory footprint to appear to grow continuously as the block cache filled up, leading to user confusion about memory "ballooning." The lower default provides more predictable memory behavior out of the box.

2. **Performance-optimized allocation**: Memtables have a much larger impact on write performance than block cache does on read performance in typical workloads. By allocating 85% of RocksDB memory to memtables, the new defaults prioritize the more performance-critical component.

Thanks to the variety of performance improvements in v1.6, Restate now delivers better performance with less memory than before.

**Impact on Users:**
- **New deployments**: Will use the new memory-efficient defaults automatically
- **Existing deployments**: Will adopt new defaults on upgrade, resulting in lower memory usage
- **Custom configurations**: Any explicitly configured `rocksdb-total-memory-size` or `rocksdb-total-memtables-ratio` values will continue to work as before

**Migration Guidance:**

The new defaults should work well for most workloads. However, if you have a read-heavy workload that benefits from a larger block cache, or if you were relying on the previous memory allocation, you can restore the old behavior:

```toml
[common]
rocksdb-total-memory-size = "6GiB"
rocksdb-total-memtables-ratio = 0.5
```

For systems with more available memory that want to maximize performance, consider tuning these values based on your specific workload characteristics and available RAM.

---

#### Abort Timeout Default Changed

The default value for `abort-timeout` has been changed from **1 minute (60 seconds)** to **10 minutes (600 seconds)**.

**Why This Matters:**

The abort timeout is a critical configuration that guards against stalled service/handler invocations. This timer:
- Starts after the `inactivity-timeout` has expired and the service has been asked to gracefully terminate
- Forces termination of the invocation when it expires
- **Can potentially interrupt user code**

**Impact on Users:**
- **Existing deployments**: Services that previously had invocations forcefully terminated after 1 minute of graceful shutdown time will now have 10 minutes
- **New deployments**: Will use the new 10-minute default automatically
- **Custom configurations**: Any explicitly configured `abort-timeout` values in configuration files will continue to work as before and are not affected by this change

If you need to revert to the previous behavior, you can explicitly configure:
```toml
[worker.invoker]
abort-timeout = "1m"
```

**Related Issues:**
- [#3961](https://github.com/restatedev/restate/issues/3961): Change defaults of abort timeout
- [#2761](https://github.com/restatedev/restate/issues/2761): Rethink inactivity timeout under high load

---

#### Query Engine Memory Budget Reduced

The default memory budget for the SQL query engine has been reduced from **4 GiB** to **1 GiB**.

**Why This Matters:**

The previous 4 GiB default was required due to inefficient queries in earlier versions. Internal optimizations, particularly streaming of underlying iterators, have significantly reduced memory requirements. This change lowers the default memory footprint of Restate.

**Impact on Users:**

- **Lower memory usage**: Default Restate deployments will use less memory

**Migration Guidance:**

If you experience slower query performance, you can restore the previous behavior by configuring a larger memory budget:

```toml
[admin.query-engine]
memory-size = "4GiB"
```

Or via environment variable:

```bash
RESTATE_ADMIN__QUERY_ENGINE__MEMORY_SIZE=4GiB
```

**Related Issues:**
- [#4240](https://github.com/restatedev/restate/pull/4240): Change default memory budget of query engine to 1 GiB

---

### Observability

#### Grafana Dashboards

New Grafana dashboards are available for monitoring Restate clusters:

| Dashboard | Description |
|-----------|-------------|
| **Restate: Overview** | High-level cluster health, resources, and throughput |
| **Restate: Internals** | Deep-dive into Bifrost, Invoker, RocksDB, and more |

The dashboards are designed for multi-node clusters with per-node breakdown and are linked together for easy navigation.

**Overview Dashboard sections:**
- Cluster Health & Errors
- Resources & Memory (JEMalloc, write buffer, RocksDB storage)
- Ingress Traffic (HTTP request rate, latency, Kafka ingress)
- Processing Pipeline (Invoker tasks, partition commands, Bifrost throughput)

**Internals Dashboard sections:**
- Errors & Diagnostics
- Ingress (HTTP & Kafka)
- Invoker (Detailed)
- Bifrost - Log System
- Partition Processor
- RocksDB
- Metadata Server

Find the dashboards in [`monitoring/grafana/`](https://github.com/restatedev/restate/tree/main/monitoring/grafana) or see the README for installation instructions.

**Related Issues:**
- [#4268](https://github.com/restatedev/restate/pull/4268): Add Grafana dashboards for monitoring Restate clusters

---

#### Suspended Invocation SQL Fields

New columns in `sys_invocation` and `sys_invocation_status` tables for debugging suspended invocations:
- `suspended_waiting_for_completions`: List of completion IDs the invocation is awaiting
- `suspended_waiting_for_signals`: List of signal indices the invocation is awaiting

```sql
SELECT id, suspended_waiting_for_completions, suspended_waiting_for_signals 
FROM sys_invocation 
WHERE status = 'suspended'
```

**Related Issues:**
- [#4209](https://github.com/restatedev/restate/pull/4209): Add suspended info fields to SQL

---

#### Invoker Metrics with Partition IDs

Invoker metrics are now tagged with partition IDs, enabling better attribution and debugging of per-partition behavior.

**Related Issues:**
- [#3883](https://github.com/restatedev/restate/pull/3883): Tag invoker metrics with partition ids

---

#### Kafka Consumer Lag Metrics

New metrics track Kafka consumer lag for subscriptions, helping monitor ingestion backpressure.

**Related Issues:**
- [#3779](https://github.com/restatedev/restate/pull/3779): Track Kafka consumer lag

---

#### Snapshot Metrics

New metrics for monitoring partition snapshot operations:

- `restate.partition_store.snapshots.upload.total` - Number of successful partition snapshot uploads
- `restate.partition_store.snapshots.upload.failed.total` - Number of failed partition snapshot uploads
- `restate.partition_store.snapshots.upload.duration.seconds` - Duration of partition snapshot upload operations
- `restate.partition_store.snapshots.download.duration.seconds` - Duration of partition snapshot download operations
- `restate.partition_store.snapshots.download.failed.total` - Number of failed partition snapshot download operations
- `restate.partition.snapshot_age.seconds` - Age of the latest partition snapshot in seconds

**Related Issues:**
- [#4299](https://github.com/restatedev/restate/pull/4299): Add snapshot metrics

---

#### Partition Lifecycle Metrics

New metrics for monitoring partition processor lifecycle events:

- `restate.partition.start.total` - Number of partition processor starts on this node
- `restate.partition.stop.total` - Number of partition processor stops on this node
  - Labels: `type` (`normal`, `startup-error`, `log-gap-detected`, `error`)
- `restate.partition.blocked_flare` - Flare indicating a partition is blocked from starting
  - Labels: `reason` (`version_barrier`, `snapshot-unavailable`)

**Related Issues:**
- [#4300](https://github.com/restatedev/restate/pull/4300): Cover partition stop/start events with metrics

---

#### jemalloc Memory Metrics and Purge Endpoint

Added jemalloc memory statistics as Prometheus metrics and a new debug endpoint to manually trigger memory purging.

**New Metrics:**
- `restate_jemalloc_allocated_bytes` - Total bytes allocated by the application
- `restate_jemalloc_active_bytes` - Total bytes in active pages (multiple of page size)
- `restate_jemalloc_metadata_bytes` - Bytes dedicated to jemalloc metadata
- `restate_jemalloc_mapped_bytes` - Total bytes in chunks mapped for the application
- `restate_jemalloc_retained_bytes` - Bytes in virtual memory mappings retained (not returned to OS)
- `restate_jemalloc_resident_bytes` - Maximum bytes in physically resident data pages

**New Debug Endpoint:**
- `POST /debug/jemalloc/purge` or `PUT /debug/jemalloc/purge` - Triggers jemalloc arena purging to release retained memory back to the OS

**Why This Matters:**

jemalloc retains memory in "dirty pages" for performance optimization, which can make it appear that the process is using more memory than actually needed. The new metrics provide visibility into this behavior, and the purge endpoint allows operators to manually reclaim retained memory when needed.

**Impact on Users:**

- **Improved observability**: The jemalloc metrics help distinguish between actual memory usage (`allocated`) and memory held by the allocator (`resident`, `retained`)
- **Manual memory management**: The purge endpoint can be useful during debugging or when the system needs to free memory quickly
- **No action required**: These are additive features with no changes to existing behavior

**Usage:**

To check jemalloc metrics:
```bash
curl http://localhost:9070/metrics | grep jemalloc
```

To trigger a manual purge:
```bash
curl -X POST http://localhost:9070/debug/jemalloc/purge
```

The purge endpoint returns before/after statistics showing memory reclaimed.

> **Warning**: Purging is an expensive operation that forces jemalloc to immediately return unused memory to the OS. This can cause significant latency spikes as it may block allocations during the purge. Only use this for debugging or one-off memory reclamation when the system is under memory pressure. Do not call this endpoint regularly or automate it in production.

---

### Stability and Security

#### Metadata Cluster Identity Validation

Nodes now validate that they are communicating with members of the same cluster by checking the cluster fingerprint when using the replicated metadata server. This prevents accidental cross-cluster communication in shared network environments (e.g., Kubernetes clusters with misconfigured networking).

**Related Issues:**
- [#4189](https://github.com/restatedev/restate/issues/4189): Harden metadata server to not accept incoming messages from other clusters

---

#### Metadata Size Limits

Soft (80%) and hard (95%) size limits are now enforced on metadata objects relative to the gRPC message size limit:
- **Soft limit**: Logs a warning with guidance (e.g., "Remove unused deployments to keep schema metadata manageable")
- **Hard limit**: Rejects writes to prevent metadata from becoming too large to synchronize

**Related Issues:**
- [#4093](https://github.com/restatedev/restate/pull/4093): Metadata object size limits

---

#### Memory Limit Warnings

On Linux containers, Restate automatically detects cgroup memory limits and emits an error if `rocksdb-total-memory-size` is too high:
- Error if > 100% of container memory (guaranteed OOM)
- Error if > 90% of container memory (OOM risk)

Recommendation: Set `RESTATE_ROCKSDB_TOTAL_MEMORY_SIZE` to 20-50% of container memory.

**Related Issues:**
- [#3925](https://github.com/restatedev/restate/pull/3925): Memory limit warnings

---

### CLI Improvements

#### Service Configuration View

`restate service config view` now displays complete service configuration including:
- Retry policy settings (max attempts, intervals, on-max-attempts behavior)
- Lazy state setting
- Per-handler configuration overrides

**Related Issues:**
- [#3945](https://github.com/restatedev/restate/pull/3945): Service config view improvements

---

#### Deployment `--extra` Flag

`restate deployment describe` and `restate deployment list` now support `--extra` to optionally show deployment status and active invocation counts. By default, these expensive queries are skipped for better performance.

```bash
restate deployment list --extra
```

**Related Issues:**
- [#3875](https://github.com/restatedev/restate/pull/3875): Deployment --extra flag

---

### Helm Chart

#### `podLabels` Support

Added support for custom pod labels in the Helm chart:
```yaml
podLabels:
  app.kubernetes.io/team: platform
```

**Related Issues:**
- [#3812](https://github.com/restatedev/restate/pull/3812): Helm chart podLabels support

---

## Bug Fixes

### Kafka

**Kafka `group.id` persistence fixed**

The `group.id` option configured in a Kafka cluster's `additional_options` is now correctly persisted to subscription metadata. Previously, the value was validated but not stored, which could cause issues when inspecting subscriptions or if cluster configuration changed.

- [#3919](https://github.com/restatedev/restate/pull/3919)

---

### Invocations

**Restart-as-new source tracking**

Restarted invocations now correctly show their origin. The `sys_invocation` table displays `invoked_by = 'restart_as_new'` and the `restarted_from` column contains the original invocation ID. Previously, restarted invocations were incorrectly marked as originating from `ingress`.

- [#3885](https://github.com/restatedev/restate/pull/3885)

**Invoker crash handling**

Improved handling of unexpected invoker crashes to prevent panics and incorrect state transitions. Retry timers are now properly fenced to avoid race conditions.

- [#4102](https://github.com/restatedev/restate/issues/4102)
- [#4085](https://github.com/restatedev/restate/issues/4085)

---

### Memory

**Memory leak in batch sender fixed**

Fixed a memory leak in the internal batch sender component.

- [#4124](https://github.com/restatedev/restate/pull/4124)

---

### CLI

**Query quote fix**

Fixed a misplaced quote in CLI SQL queries that could cause syntax errors.

- [#4116](https://github.com/restatedev/restate/pull/4116)

**Active invocations query fix**

Fixed queries for active invocations on a deployment that could return incorrect results.

- [#4115](https://github.com/restatedev/restate/pull/4115)

---

### UI

**Magic links fix**

Fixed an issue where magic links in the UI server were not working correctly.

- [#3946](https://github.com/restatedev/restate/pull/3946)

---

### Metadata

**H2 protocol errors now retryable**

H2 protocol errors in the metadata store client are now treated as retryable, improving resilience during transient network issues.

- [#4165](https://github.com/restatedev/restate/pull/4165)
